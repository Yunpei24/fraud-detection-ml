# ğŸ“Š DATA MODULE - DÃ©tails d'ImplÃ©mentation

## ğŸ¯ Vue d'ensemble

Le module **d---

## ğŸ“Š Flux de DonnÃ©es - PRODUCTION (v1.1.0)

### Real-time Pipeline

```
Azure Event Hub / Kafka Topic
    â†“ (JSON transaction events)
[EventHubConsumer / KafkaConsumer]
    â†“
[RealtimePipeline.process_event()]
    â†“
[ProductionSchemaValidator.validate_batch()]
    â”œâ”€ Check required fields (10+)
    â”œâ”€ Validate types
    â”œâ”€ Validate business rules
    â””â”€ âœ… Valid / âŒ Invalid â†’ Log & Skip
    â†“
[Buffer (batch_size=100 OR flush_interval_seconds=60)]
    â†“
[Transformation]
    â”œâ”€ DataCleaner.clean_pipeline()
    â”œâ”€ FeatureEngineer.engineer_features()
    â””â”€ Create 28+ features for ML
    â†“
[Storage]
    â”œâ”€ DatabaseService (insert_transactions)
    â”œâ”€ FeatureStoreService (save_features)
    â””â”€ DataLakeService (save_parquet)
    â†“
[Metrics]
    â””â”€ Prometheus (transactions_processed_total, etc)
```

### Batch Pipeline

```
Data Source (Event Hub batch export, Parquet, SQL Query)
    â†“
[BatchPipeline.load_data()]
    â†“
[ProductionSchemaValidator.validate_batch()]
    â””â”€ Rows invalid â†’ Log & Skip
    â†“
[DataCleaner.clean_pipeline()]
    â”œâ”€ Remove duplicates
    â”œâ”€ Handle missing values
    â”œâ”€ Remove outliers
    â””â”€ Standardize column names
    â†“
[FeatureEngineer.engineer_features()]
    â”œâ”€ Temporal features (7)
    â”œâ”€ Amount features (3)
    â”œâ”€ Customer aggregations (7)
    â”œâ”€ Merchant aggregations (6)
    â””â”€ Interaction features (5)
    â†“
[DatabaseService.insert_transactions()]
    â””â”€ Store with predictions in database
    â†“
[Metrics]
    â””â”€ Report statistics to Prometheus
```

---

## âœ… Production Schema (v1.1.0)

**Requiert**: DonnÃ©es depuis Event Hub/Kafka avec champs PRODUCTION

```json
{
  "transaction_id": "TXN123456",
  "customer_id": "CUST001", 
  "merchant_id": "MRCH001",
  "amount": 125.50,
  "currency": "USD",
  "transaction_time": "2025-10-19T14:30:00Z",
  "customer_zip": "12345",
  "merchant_zip": "54321",
  "customer_country": "US",
  "merchant_country": "US",
  "device_id": "DEV789",
  "session_id": "SES456",
  "ip_address": "192.168.1.1",
  "mcc": 4111,
  "transaction_type": "PURCHASE",
  "is_disputed": false,
  "source_system": "mobile"
}
```

**REMARQUE IMPORTANTE**: 
- Kaggle CSV format (Time, V1-V28, Amount, Class) a Ã©tÃ© utilisÃ© UNIQUEMENT en dÃ©veloppement
- Tout le code Kaggle-specific a Ã©tÃ© supprimÃ© (v1.1.0)
- Le systÃ¨me en PRODUCTION utilise EXCLUSIVEMENT cette structure Event Hub/** est le cÅ“ur du systÃ¨me de dÃ©tection de fraude. Il gÃ¨re:
- âœ… **Ingestion** - RÃ©ception de donnÃ©es depuis Event Hub/Kafka
- âœ… **Validation** - VÃ©rification PRODUCTION du schÃ©ma Event Hub/Kafka et qualitÃ© des donnÃ©es
- âœ… **Transformation** - Nettoyage et ingÃ©nierie des features
- âœ… **Stockage** - Persistance en base de donnÃ©es et Data Lake
- âœ… **Monitoring** - MÃ©triques et santÃ© du systÃ¨me

**Important**: Le systÃ¨me fonctionne **EXCLUSIVEMENT** avec le schÃ©ma PRODUCTION (Event Hub/Kafka).
Le CSV Kaggle Ã©tait utilisÃ© uniquement pour comprendre la structure des donnÃ©es en phase de dÃ©veloppement.
Tout le code Kaggle-specific a Ã©tÃ© supprimÃ© (v1.1.0, October 2025).

---

## ğŸ“ Arborescence DÃ©taillÃ©e

```
data/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ settings.py          âš™ï¸ Configuration centralisÃ©e
â”‚   â”‚   â””â”€â”€ constants.py         ğŸ“‹ Constantes globales
â”‚   â”‚
â”‚   â”œâ”€â”€ ingestion/              ğŸ”Œ COUCHE D'INGESTION
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ event_hub.py        â†’ Azure Event Hub (streaming)
â”‚   â”‚   â””â”€â”€ kafka.py            â†’ Kafka (alternative)
â”‚   â”‚
â”‚   â”œâ”€â”€ validation/             âœ”ï¸ COUCHE DE VALIDATION
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ schema.py           â†’ Validation du schÃ©ma transactionnel
â”‚   â”‚   â”œâ”€â”€ quality.py          â†’ ContrÃ´le qualitÃ© des donnÃ©es
â”‚   â”‚   â””â”€â”€ anomalies.py        â†’ DÃ©tection d'anomalies statistiques
â”‚   â”‚
â”‚   â”œâ”€â”€ transformation/         ğŸ”„ COUCHE DE TRANSFORMATION
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ cleaner.py          â†’ Nettoyage et prÃ©traitement
â”‚   â”‚   â”œâ”€â”€ features.py         â†’ IngÃ©nierie des features
â”‚   â”‚   â””â”€â”€ aggregator.py       â†’ AgrÃ©gations batch
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/               ğŸ’¾ COUCHE DE STOCKAGE
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ database.py         â†’ SQL Server/PostgreSQL
â”‚   â”‚   â”œâ”€â”€ data_lake.py        â†’ Azure Data Lake (big data)
â”‚   â”‚   â””â”€â”€ feature_store.py    â†’ Cache Redis/Feature Store
â”‚   â”‚
â”‚   â”œâ”€â”€ monitoring/            ğŸ“ˆ COUCHE DE MONITORING
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ metrics.py          â†’ Prometheus metrics
â”‚   â”‚   â””â”€â”€ health.py           â†’ Health checks
â”‚   â”‚
â”‚   â””â”€â”€ pipelines/             ğŸš€ PIPELINES D'ORCHESTRATION
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ realtime_pipeline.py   â†’ Streaming en temps rÃ©el
â”‚       â””â”€â”€ batch_pipeline.py      â†’ Traitement batch
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py            â†’ Pytest fixtures
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ test_schema.py      â†’ Tests validation
â”‚   â”‚   â”œâ”€â”€ test_quality.py     â†’ Tests qualitÃ©
â”‚   â”‚   â”œâ”€â”€ test_cleaner.py     â†’ Tests nettoyage
â”‚   â”‚   â””â”€â”€ test_features.py    â†’ Tests features
â”‚   â””â”€â”€ integration/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ test_data_pipeline.py â†’ Tests e2e
â”‚
â”œâ”€â”€ examples.py                ğŸ“š Exemples d'utilisation
â”œâ”€â”€ requirements.txt           ğŸ“¦ DÃ©pendances Python
â”œâ”€â”€ schema.sql                 ğŸ—„ï¸ SchÃ©ma base de donnÃ©es
â””â”€â”€ README.md                  ğŸ“– Documentation
```

---

## ğŸ”Œ Flux de DonnÃ©es (Data Flow)

### Real-time Pipeline

```
Event Hub/Kafka
    â†“
  [EventHubConsumer]
    â†“
[RealtimePipeline]
    â†“
[Validation] â†’ âœ… Valid / âŒ Invalid
    â†“
[Buffer]
    â†“ (batch_size OR flush_interval)
[Transformation]
    â”œâ”€ Clean
    â””â”€ Features
    â†“
[Database] + [Feature Store] + [Data Lake]
```

### Batch Pipeline

```
CSV / Parquet / SQL Query
    â†“
[BatchPipeline.load_data()]
    â†“
[SchemaValidator] â†’ Rows invalid rejetÃ©s
    â†“
[DataCleaner]
    â”œâ”€ Remove duplicates
    â”œâ”€ Handle missing values
    â”œâ”€ Remove outliers
    â””â”€ Standardize names
    â†“
[FeatureEngineer]
    â”œâ”€ Temporal features
    â”œâ”€ Amount features
    â”œâ”€ Customer aggregations
    â”œâ”€ Merchant aggregations
    â””â”€ Interaction features
    â†“
[DatabaseService] â†’ Insert transactions + predictions
    â†“
[Metrics] â†’ Prometheus
```

---

## ğŸ“¦ Composants Principaux

### 1ï¸âƒ£ Configuration (`config/`)

**`settings.py`**
```python
settings = Settings()
# Charge automatiquement depuis .env
# settings.azure.connection_string
# settings.database.server
# settings.cache.host
```

**`constants.py`**
```python
BATCH_SIZE = 100
MAX_RETRIES = 3
MAX_MISSING_PERCENTAGE = 0.05
VALID_CURRENCIES = ["USD", "EUR", "GBP", ...]
```

### 2ï¸âƒ£ Ingestion (`ingestion/`)

**`event_hub.py`**
```python
consumer = EventHubConsumer()
consumer.connect()
consumer.start(on_event_received=process_transaction)
```

**`kafka.py`**
```python
consumer = KafkaTransactionConsumer()
consumer.start(on_message=process_transaction)
```

### 3ï¸âƒ£ Validation (`validation/`)

**`schema.py`**
```python
# PRODUCTION SCHEMA ONLY
validator = SchemaValidator()
df_validated = validator.validate_batch(df, schema_type='production')

# Valide les donnÃ©es Event Hub/Kafka:
# - 10+ required fields (transaction_id, customer_id, merchant_id, amount, etc.)
# - Types corrects
# - RÃ¨gles mÃ©tier (montant >= 0, devise 3-lettres, pas d'IDs vides)
```

**`base_schema.py`**
```python
# Abstract base class for custom schemas
class BaseSchema(ABC):
    @property
    def required_fields(self) -> list: ...
    
    def validate_fields(self, df: pd.DataFrame) -> tuple[bool, List[str]]: ...
    def validate_types(self, df: pd.DataFrame) -> tuple[bool, Dict[str, str]]: ...
    def validate_business_rules(self, df: pd.DataFrame) -> tuple[bool, List[str]]: ...

# ProductionSchemaValidator extends BaseSchema
```

**`quality.py`**
```python
quality_checker = QualityValidator()
report = quality_checker.validate_batch(df)
# VÃ©rifie: nulls, doublons, outliers, types
```

**`anomalies.py`**
```python
anomaly_detector = AnomalyDetector()
report = anomaly_detector.run_full_analysis(df)
# DÃ©tecte: colonnes manquantes, distributions anormales, cardinality haute
```

### 4ï¸âƒ£ Transformation (`transformation/`)

**`cleaner.py`**
```python
cleaner = DataCleaner()
df_clean = cleaner.clean_pipeline(df,
    remove_dups=True,
    handle_missing=True,
    remove_outliers_flag=False
)
```

**`features.py`**
```python
engineer = FeatureEngineer()
df_features = engineer.engineer_features(df)
# CrÃ©e 28+ features:
# - Temporelles (7): hour, day_of_week, is_weekend, etc
# - Montant (3): log, squared, buckets
# - Client (7): count, avg, std, min, max, sum
# - Marchand (6): count, avg, std, min, max
# - Interaction (5): count, avg, std, max customer-merchant
```

**`aggregator.py`**
```python
aggregator = TransactionAggregator()
daily_agg = aggregator.aggregate_by_time(df, period="D")
customer_agg = aggregator.aggregate_by_customer(df)
merchant_agg = aggregator.aggregate_by_merchant(df)
```

### 5ï¸âƒ£ Stockage (`storage/`)

**`database.py`**
```python
db = DatabaseService()
db.connect()
rows_inserted = db.insert_transactions(transactions)
stats = db.get_statistics()
```

**`data_lake.py`**
```python
datalake = DataLakeService()
datalake.save_parquet(df, "transactions/2025-10-18.parquet")
df_loaded = datalake.read_parquet("transactions/2025-10-18.parquet")
datalake.save_json_lines(records, "raw/2025-10-18.jsonl")
```

**`feature_store.py`**
```python
feature_store = FeatureStoreService(backend="redis")
feature_store.save_features("CUST001", {"total_transactions": 42})
features = feature_store.get_features("CUST001")
```

### 6ï¸âƒ£ Monitoring (`monitoring/`)

**`metrics.py`**
```python
metrics = MetricsCollector()
metrics.record_transaction_processed(100)
metrics.record_ingestion_latency(0.5)
metrics.record_validation_error()
# Expose via Prometheus sur :8000/metrics
```

**`health.py`**
```python
monitor = HealthMonitor()
monitor.check_database_connection(db_service)
monitor.check_data_lake_connection(datalake)
health = monitor.get_overall_health()
```

### 7ï¸âƒ£ Pipelines (`pipelines/`)

**`realtime_pipeline.py`**
```python
pipeline = RealtimePipeline(batch_size=100, flush_interval_seconds=60)
pipeline.process_event(event, validator, cleaner, db_service)
pipeline.shutdown(cleaner, db_service)
```

**`batch_pipeline.py`**
```python
pipeline = BatchPipeline()
stats = pipeline.execute(
    input_source="transactions.csv",
    validator=validator,
    cleaner=cleaner,
    feature_engineer=engineer,
    storage_service=db_service
)
```

---

## ğŸ§ª Tests

### Structure
```
tests/
â”œâ”€â”€ conftest.py               # Fixtures partagÃ©es
â”œâ”€â”€ unit/                     # Tests unitaires
â”‚   â”œâ”€â”€ test_schema.py       # Validation schema
â”‚   â”œâ”€â”€ test_quality.py      # QualitÃ© donnÃ©es
â”‚   â”œâ”€â”€ test_cleaner.py      # Nettoyage
â”‚   â””â”€â”€ test_features.py     # Features
â””â”€â”€ integration/             # Tests d'intÃ©gration
    â””â”€â”€ test_data_pipeline.py # Pipeline end-to-end
```

### ExÃ©cution
```bash
# Tous les tests
pytest data/tests/ -v

# Tests spÃ©cifiques
pytest data/tests/unit/test_schema.py -v

# Avec couverture
pytest data/tests/ --cov=data/src --cov-report=html

# Only integration
pytest data/tests/integration/ -v
```

---

## ğŸ“Š MÃ©triques Prometheus

```
fraud_detection_data_transactions_processed_total
fraud_detection_data_transactions_ingested_total
fraud_detection_data_validation_errors_total
fraud_detection_data_data_quality_issues_total

fraud_detection_data_ingestion_latency_seconds
fraud_detection_data_processing_latency_seconds
fraud_detection_data_validation_latency_seconds

fraud_detection_data_active_connections
fraud_detection_data_queue_size
fraud_detection_data_last_processed_timestamp
```

---

## ğŸ”§ Configuration (.env)

```bash
# Azure
AZURE_STORAGE_CONNECTION_STRING=...
EVENT_HUB_CONNECTION_STRING=...

# Database
DB_SERVER=localhost
DB_NAME=frauddb
DB_USER=sa
DB_PASSWORD=...

# Redis
REDIS_HOST=localhost
REDIS_PORT=6379

# Monitoring
LOG_LEVEL=INFO
PROMETHEUS_PORT=8000
```

---

## ğŸ“ˆ Points de Performance

| Composant | Latence Cible | Throughput |
|-----------|----------------|-----------|
| Validation | < 10ms | 10K tx/sec |
| Nettoyage | < 50ms/batch | 100K rows/sec |
| Features | < 100ms/batch | 50K rows/sec |
| Stockage DB | < 500ms/batch | 1K rows/sec |
| Data Lake | < 2s/batch | 100K rows/sec |

---

## ğŸš€ Prochaines Ã‰tapes

AprÃ¨s le module `data/`, implÃ©menter:

1. **training/** - EntraÃ®nement des modÃ¨les
2. **api/** - Serveur FastAPI pour infÃ©rence
3. **drift/** - DÃ©tection de dÃ©rive conceptuelle
4. **airflow/** - Orchestration des workflows
5. **tests/** - Suite de tests globale
6. **CI/CD** - GitHub Actions workflows

---

## ğŸ“š Ressources

- [Pandas Docs](https://pandas.pydata.org/)
- [Azure SDK Python](https://github.com/Azure/azure-sdk-for-python)
- [SQLAlchemy](https://docs.sqlalchemy.org/)
- [Pytest](https://docs.pytest.org/)
- [Prometheus Client](https://github.com/prometheus/client_python)

---

**Module**: Data Ingestion & Processing  
**Version**: 1.1.0 (Production-Ready, Kaggle-cleanup complete)  
**Status**: âœ… Production-Ready  
**Created**: October 2025
**Last Updated**: October 19, 2025

### Derniers Changes (v1.1.0)

âœ… **Suppression Kaggle-Specific Code**
- Removed 10+ fichiers avec "kaggle" dans le nom (~1,500 lignes)
- Removed src/adapters/ directory
- Removed synthetic data generation code

âœ… **ImplÃ©mentation ProductionSchemaValidator**
- Validates Event Hub/Kafka messages exclusivement
- 10+ required fields support
- Business rules validation
- 14 comprehensive tests (all passing)

âœ… **Refactoring Complet**
- Abstract base classes (BaseSchema, BasePipeline, BaseDataLoader)
- Production-only architecture
- All 36 tests passing (100%)
- verify.py updated to use new API

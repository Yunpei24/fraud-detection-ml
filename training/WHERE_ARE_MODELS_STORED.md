# ğŸ“¦ Where are Models Stored After Training?

**Date:** November 4, 2025  
**After fixing the MLflow registration bug**

---

## ğŸ¯ Overview

After executing the DAG `01_training_pipeline`, models are stored in **3 different locations**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STORAGE LOCATIONS AFTER TRAINING                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1ï¸âƒ£  MLflow Tracking Server (Runs)
    ğŸ“ Location: MLflow container - http://localhost:5001
    ğŸ“‚ Path: /mlflow/artifacts/<experiment_id>/<run_id>/
    ğŸ“Š Content: Model artifacts, metrics, parameters, plots
    ğŸ”„ Lifecycle: Permanent (until manual deletion)

2ï¸âƒ£  MLflow Model Registry
    ğŸ“ Location: MLflow container - Model Registry
    ğŸ“‚ Path: models:/fraud_detection_<model_name>/Staging
    ğŸ“Š Content: Model versions with stage (None â†’ Staging â†’ Production)
    ğŸ”„ Lifecycle: Versioning with stage promotion

3ï¸âƒ£  Training Container (temporary)
    ğŸ“ Location: fraud-training container
    ğŸ“‚ Path: /app/models/ (empty after training)
    ğŸ“Š Content: None (models not saved locally)
    ğŸ”„ Lifecycle: Ephemeral (lost on restart)
```

---

## ğŸ” 1. MLflow Tracking Server (Runs)

### **Where are the artifacts?**

Models are logged in **MLflow Tracking** during training:

```bash
# MLflow Container
docker exec fraud-mlflow ls -lah /mlflow/artifacts/
```

**Artifacts structure:**
```
/mlflow/artifacts/
â”œâ”€â”€ 1/                          # Experiment ID (fraud_detection_training)
â”‚   â”œâ”€â”€ <run_id_1>/            # Run: register_xgboost
â”‚   â”‚   â”œâ”€â”€ artifacts/
â”‚   â”‚   â”‚   â”œâ”€â”€ model/         # XGBoost model
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ MLmodel
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ model.pkl
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â”‚   â”‚   â””â”€â”€ xgboost_metadata.json
â”‚   â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â””â”€â”€ params/
â”‚   â”‚
â”‚   â”œâ”€â”€ <run_id_2>/            # Run: register_random_forest
â”‚   â”‚   â””â”€â”€ artifacts/
â”‚   â”‚       â””â”€â”€ model/         # Random Forest model
â”‚   â”‚
â”‚   â”œâ”€â”€ <run_id_3>/            # Run: register_neural_network
â”‚   â”‚   â””â”€â”€ artifacts/
â”‚   â”‚       â””â”€â”€ model/         # Neural Network model
â”‚   â”‚
â”‚   â””â”€â”€ <run_id_4>/            # Run: register_isolation_forest
â”‚       â””â”€â”€ artifacts/
â”‚           â””â”€â”€ model/         # Isolation Forest model
```

### **How to view them?**

**Method 1: MLflow Web Interface**
```bash
# Open in browser
http://localhost:5001
```

**Method 2: CLI in container**
```bash
# List experiments
docker exec fraud-training python -c "
import mlflow
mlflow.set_tracking_uri('http://mlflow:5000')
client = mlflow.MlflowClient()

experiments = client.search_experiments()
for exp in experiments:
    print(f'Experiment: {exp.name} (ID: {exp.experiment_id})')
    runs = client.search_runs([exp.experiment_id], max_results=10)
    print(f'  Total runs: {len(runs)}')
    for run in runs:
        print(f'    - {run.info.run_name} ({run.info.run_id[:8]}...)')
"
```

**Method 3: Check artifacts directly**
```bash
# Find experiment ID
docker exec fraud-mlflow ls -lah /mlflow/artifacts/

# List runs in experiment
docker exec fraud-mlflow ls -lah /mlflow/artifacts/1/

# View artifacts of a specific run
docker exec fraud-mlflow ls -lah /mlflow/artifacts/1/<run_id>/artifacts/model/
```

---

## ğŸ·ï¸ 2. MLflow Model Registry

### **What is the Model Registry?**

The **Model Registry** is a **versioning database** for ML models:
- Stores model **metadata** (name, version, stage, description)
- Points to **artifacts** in the Tracking Server
- Enables **promotion** between stages (None â†’ Staging â†’ Production)

### **Where are models in the Registry?**

Models are **registered under names**:

```
ğŸ“¦ MLflow Model Registry
â”œâ”€â”€ fraud_detection_xgboost
â”‚   â””â”€â”€ Version 1 (Stage: Staging)
â”‚       â”œâ”€â”€ Run ID: <run_id>
â”‚       â””â”€â”€ Source: runs:/<run_id>/model
â”‚
â”œâ”€â”€ fraud_detection_random_forest
â”‚   â””â”€â”€ Version 1 (Stage: Staging)
â”‚
â”œâ”€â”€ fraud_detection_neural_network
â”‚   â””â”€â”€ Version 1 (Stage: Staging)
â”‚
â””â”€â”€ fraud_detection_isolation_forest
    â””â”€â”€ Version 1 (Stage: Staging)
```

### **How to view them?**

**Method 1: MLflow Web Interface**
```bash
http://localhost:5001/#/models
```

**Method 2: Python API**
```bash
docker exec fraud-training python -c "
import mlflow
mlflow.set_tracking_uri('http://mlflow:5000')
client = mlflow.MlflowClient()

# List all registered models
models = client.search_registered_models()

print(f'ğŸ“¦ {len(models)} models registered in MLflow:\n')
for model in models:
    print(f'Model: {model.name}')
    
    # List versions
    versions = client.search_model_versions(f\"name='{model.name}'\")
    for v in versions:
        print(f'  - Version {v.version}')
        print(f'    Stage: {v.current_stage}')
        print(f'    Run ID: {v.run_id}')
        print(f'    Source: {v.source}')
    print()
"
```

**Method 3: Load a model from Registry**
```python
import mlflow

mlflow.set_tracking_uri('http://mlflow:5000')

# Load a model from Registry
model_uri = "models:/fraud_detection_xgboost/Staging"
model = mlflow.pyfunc.load_model(model_uri)

print(f"Model loaded: {model}")
```


### **How to verify?**

```bash
# Check models directory
docker exec fraud-training ls -lah /app/models/
# Output: total 8.0K (empty)

# Models are in MLflow, not locally
docker exec fraud-training python -c "
import mlflow
mlflow.set_tracking_uri('http://mlflow:5000')
client = mlflow.MlflowClient()
models = client.search_registered_models()
print(f'{len(models)} models in MLflow Registry')
"
```

---

## ğŸ”„ Complete Flow: Training â†’ MLflow â†’ API

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  MODEL LIFECYCLE FLOW                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: TRAINING (DAG 01_training_pipeline)
â”œâ”€ Container: fraud-training
â”œâ”€ Script: src/pipelines/training_pipeline.py
â”œâ”€ Actions:
â”‚  â”œâ”€ Train XGBoost, Random Forest, Neural Network, Isolation Forest
â”‚  â”œâ”€ Evaluate models on test set
â”‚  â”œâ”€ For each model:
â”‚  â”‚   â”œâ”€ Create MLflow run: register_<model_name>
â”‚  â”‚   â”œâ”€ Log model: mlflow.sklearn.log_model(model, "model")
â”‚  â”‚   â”œâ”€ Register: mlflow.register_model("runs:/<run_id>/model", name)
â”‚  â”‚   â””â”€ Transition stage: Staging
â”‚  â””â”€ Log metadata and plots
â””â”€ Result: 4 models in MLflow Registry (Stage: Staging)

STEP 2: STORAGE (MLflow)
â”œâ”€ Location 1: Tracking Server
â”‚  â”œâ”€ Path: /mlflow/artifacts/1/<run_id>/artifacts/model/
â”‚  â””â”€ Content: model.pkl, MLmodel, requirements.txt
â”‚
â””â”€ Location 2: Model Registry
   â”œâ”€ Name: fraud_detection_<model_name>
   â”œâ”€ Version: 1
   â”œâ”€ Stage: Staging
   â””â”€ Source: runs:/<run_id>/model

STEP 3: DEPLOYMENT (DAG 05_model_deployment_canary_http)
â”œâ”€ Container: airflow-worker (calls API)
â”œâ”€ Script: api/scripts/deploy_canary.py
â”œâ”€ Actions:
â”‚  â”œâ”€ Load models from MLflow: models:/fraud_detection_*/Staging
â”‚  â”œâ”€ Save to Azure File Share: /mnt/fraud-models/canary/
â”‚  â”œâ”€ Update traffic_routing.json (5% canary)
â”‚  â””â”€ API auto-reloads models
â””â”€ Result: Models deployed in API container

STEP 4: SERVING (API)
â”œâ”€ Container: fraud-api
â”œâ”€ Path: /mnt/fraud-models/champion/ (or canary/)
â”œâ”€ Auto-reload: Timestamp-based detection
â””â”€ Endpoints:
   â”œâ”€ POST /api/v1/predict (predictions)
   â”œâ”€ GET /api/v1/explain/models (list models)
   â””â”€ POST /api/v1/explain/shap (SHAP explanations)
```

---

## ğŸ› ï¸ Diagnostic Commands

### **1. Check that models are in MLflow Registry**

```bash
docker exec fraud-training python -c "
import mlflow
mlflow.set_tracking_uri('http://mlflow:5000')
client = mlflow.MlflowClient()

models = client.search_registered_models()
if not models:
    print('âŒ No models in Registry - Training failed or bug in registration')
else:
    print(f'âœ… {len(models)} models in Registry')
    for model in models:
        versions = client.search_model_versions(f\"name='{model.name}'\")
        print(f'  - {model.name}: {len(versions)} version(s)')
        for v in versions:
            print(f'      Version {v.version} ({v.current_stage})')
"
```

### **2. Check artifacts in MLflow**

```bash
# Find experiment ID
docker exec fraud-mlflow ls /mlflow/artifacts/

# List runs
docker exec fraud-mlflow ls /mlflow/artifacts/1/

# View artifacts of a run
docker exec fraud-mlflow find /mlflow/artifacts/1/ -name "model.pkl" | head -5
```

### **3. Load a model from MLflow**

```bash
docker exec fraud-training python -c "
import mlflow

mlflow.set_tracking_uri('http://mlflow:5000')

# Load XGBoost model in Staging
model_uri = 'models:/fraud_detection_xgboost/Staging'
try:
    model = mlflow.pyfunc.load_model(model_uri)
    print(f'âœ… Model loaded successfully from {model_uri}')
    print(f'   Type: {type(model)}')
except Exception as e:
    print(f'âŒ Failed to load model: {e}')
"
```

### **4. Check MLflow Docker volume**

```bash
# Inspect mlflow_artifacts volume
docker volume inspect fraud-detection-ml_mlflow_artifacts

# View volume size
docker system df -v | grep mlflow_artifacts
```

---

## ğŸ“‹ Location Summary

| Where? | Path | Content | When? |
|--------|------|---------|-------|
| **MLflow Tracking** | `/mlflow/artifacts/1/<run_id>/` | Model artifacts (.pkl, MLmodel) | After `log_model()` |
| **MLflow Registry** | `models:/fraud_detection_*/Staging` | Metadata + pointer to artifacts | After `register_model()` |
| **Training Container** | `/app/models/` | âŒ Empty (not used) | Never |
| **API Container** | `/mnt/fraud-models/champion/` | Deployed models | After `deploy_canary.py` |

---

## ğŸš€ Next Steps

Now that the bug is fixed, **re-run the training DAG**:

1. **Trigger the DAG in Airflow UI**
   ```bash
   # Open Airflow
   http://localhost:8080
   
   # DAG: 01_training_pipeline
   # Click: Trigger DAG
   ```

2. **Wait for training to complete** (~15-30 minutes)

3. **Verify that models are registered**
   ```bash
   docker exec fraud-training python -c "
   import mlflow
   mlflow.set_tracking_uri('http://mlflow:5000')
   client = mlflow.MlflowClient()
   models = client.search_registered_models()
   print(f'{len(models)} models registered')
   "
   ```

4. **Deploy via canary DAG**
   ```bash
   # DAG: 05_model_deployment_canary_http
   # Click: Trigger DAG
   ```

5. **Verify in API**
   ```bash
   curl -X GET "http://localhost:8000/api/v1/explain/models" \
     -H "Authorization: Bearer $TOKEN"
   ```

---

**Questions?** Check out:
- [MODEL_STORAGE_EXPLAINED.md](../api/MODEL_STORAGE_EXPLAINED.md) - API-side storage
- [DEPLOYMENT_API.md](../api/DEPLOYMENT_API.md) - Canary deployment
- [README.md](README.md) - Training documentation